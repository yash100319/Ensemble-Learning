{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNcXBhMNPyUj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Ensemble Learning\n",
        "#Theoretical\n",
        "1.  Can we use Bagging for regression problems ?\n",
        "- Multiple subsets of the training data are created using bootstrapping (sampling with replacement).\n",
        "\n",
        "A base regressor (e.g., Decision Tree Regressor) is trained on each subset.\n",
        "\n",
        "The final prediction is made by averaging the predictions of all the base regressors.\n",
        "2. What is the difference between multiple model training and single model training ?\n",
        "- The difference between multiple model training and single model training lies in how many models are used during the learning process and how predictions are made\n",
        "3.  Explain the concept of feature randomness in Random Forest ?\n",
        "- Feature randomness is a key concept that makes Random Forest different from standard bagging and improves its performance.\n",
        "\n",
        "In Random Forest, at each split in a decision tree, instead of considering all features, only a random subset of features is considered when deciding the best split.\n",
        "4. What is OOB (Out-of-Bag) Score ?\n",
        "- OOB (Out-of-Bag) Score is an internal validation method used in Bagging algorithms, especially in Random Forests, to estimate model performance without needing a separate validation set or cross-validation.\n",
        "5. 2 How can you measure the importance of features in a Random Forest model ?\n",
        "- This is the default method used in most implementations like sklearn.\n",
        "\n",
        "It measures how much each feature decreases the impurity (e.g., Gini or entropy) across all trees in the forest.\n",
        "\n",
        "Steps:\n",
        "\n",
        "For every feature, calculate the total decrease in impurity it brings when used to split a node.\n",
        "\n",
        "Average this decrease over all trees in the forest.\n",
        "6. Explain the working principle of a Bagging Classifier ?\n",
        "- The Bagging Classifier (short for Bootstrap Aggregating) is an ensemble learning technique that aims to improve the stability and accuracy of machine learning algorithms, primarily decision trees\n",
        "1. Bootstrap Sampling\n",
        "From the original dataset (with\n",
        "ùëõ\n",
        "n samples), multiple new training datasets are created by random sampling with replacement.\n",
        "\n",
        "Each of these is called a bootstrap sample and typically has the same size as the original dataset.\n",
        "\n",
        "2. Train Base Learners\n",
        "A base classifier (often a decision tree) is trained independently on each bootstrap sample.\n",
        "\n",
        "Since each sample is different, the base learners will vary ‚Äî even if the same algorithm is used.\n",
        "7. How do you evaluate a Bagging Classifier‚Äôs performance\n",
        "- Evaluating a Bagging Classifier‚Äôs performance is similar to evaluating any supervised learning model, but there are a few best practices to consider due to its ensemble nature.\n",
        "\n",
        "1. Train-Test Split or Cross-Validation\n",
        "Train-test split: Divide data into training and testing sets.\n",
        "\n",
        "k-fold Cross-Validation: More robust ‚Äî averages results across k different train-test splits.\n",
        "8. How does a Bagging Regressor work ?\n",
        "- A Bagging Regressor works on the same principles as a Bagging Classifier, but it's used for regression tasks, where the goal is to predict a continuous value rather than a class label.\n",
        "Bootstrap Sampling\n",
        "\n",
        "Generate multiple datasets by sampling with replacement from the original training set.\n",
        "\n",
        "Each sample is the same size as the original dataset but may contain duplicates.\n",
        "\n",
        "Train Base Regressors\n",
        "\n",
        "A base regressor (e.g., a Decision Tree Regressor) is trained on each bootstrap sample.\n",
        "\n",
        "Because each model sees slightly different data, they will produce different predictions\n",
        "9. What is the main advantage of ensemble techniques ?\n",
        "- The main advantage of ensemble techniques is:\n",
        "\n",
        " Improved model performance and robustness by combining multiple models.\n",
        "\n",
        "In other words, ensembles leverage the strengths of multiple models to produce more accurate, stable, and generalizable predictions than any single model alone.\n",
        "A single decision tree might overfit the training data.\n",
        "But a Random Forest (an ensemble of many trees) typically generalizes much better due to:\n",
        "\n",
        "Random feature selection\n",
        "\n",
        "Averaging many predictions\n",
        "10. What is the main challenge of ensemble methods ?\n",
        "- Computationally Intensive\n",
        "\n",
        "Training and predicting require running multiple models instead of just one, which means more CPU/GPU time and memory.\n",
        "\n",
        "Hard to Interpret\n",
        "\n",
        "It‚Äôs difficult to understand and explain how the ensemble arrives at its decisions because it aggregates many base learners.\n",
        "\n",
        "Hyperparameter Tuning\n",
        "\n",
        "More parameters to tune (number of estimators, learning rate, depth, etc.), which can complicate model optimization.\n",
        "11.  Explain the key idea behind ensemble techniques ?\n",
        "- The key idea behind ensemble techniques is:\n",
        "\n",
        "Combine multiple individual models (called base learners) to create a stronger, more accurate, and more robust overall model.\n",
        "Individual models have their own strengths and weaknesses.\n",
        "\n",
        "By aggregating their predictions, ensembles can reduce errors like bias and variance.\n",
        "\n",
        "Different models make different mistakes; combining them helps cancel out errors\n",
        "12. What is a Random Forest Classifier ?\n",
        "- A Random Forest Classifier is an ensemble learning method used for classification tasks. It builds upon the idea of bagging with decision trees and adds randomness to improve performance and reduce overfitting.\n",
        "Ensemble of Decision Trees:\n",
        "It constructs multiple decision trees during training.\n",
        "\n",
        "Bootstrap Sampling (Bagging):\n",
        "Each tree is trained on a random subset of the training data sampled with replacement.\n",
        "\n",
        "Random Feature Selection:\n",
        "When splitting nodes, each tree considers a random subset of features rather than all features. This introduces more diversity among trees.\n",
        "13. What are the main types of ensemble techniques ?\n",
        "- 1. Bagging (Bootstrap Aggregating)\n",
        "How it works:\n",
        "Train multiple base models independently on different random bootstrap samples of the training data.\n",
        "\n",
        "Goal:\n",
        "Reduce variance and avoid overfitting by averaging or voting.\n",
        "2. Boosting\n",
        "How it works:\n",
        "Train base models sequentially, where each new model tries to correct the errors of the previous ones by focusing more on difficult samples.\n",
        "\n",
        "Goal:\n",
        "Reduce bias and build a strong predictive model by combining many weak learners.\n",
        "14.  What is ensemble learning in machine learning ?\n",
        "- Ensemble learning in machine learning is a technique where multiple models (called base learners) are combined to solve a problem and improve overall performance compared to using a single model.\n",
        "Different models may make different errors.\n",
        "\n",
        "Combining their predictions helps reduce errors like bias and variance.\n",
        "\n",
        "Helps prevent overfitting and improves stability\n",
        "15. When should we avoid using ensemble methods ?\n",
        "- 1. When Interpretability Is Crucial\n",
        "Ensembles combine many models, making them hard to interpret.\n",
        "\n",
        "If you need a clear, simple explanation (e.g., medical diagnosis, regulatory settings), simpler models like decision trees or linear models might be better.\n",
        "\n",
        "2. When Computational Resources Are Limited\n",
        "Training and predicting with ensembles can be computationally expensive and memory-intensive.\n",
        "\n",
        "For real-time or low-resource environments (like mobile devices), simpler models may be preferred.\n",
        "16. How does Bagging help in reducing overfitting ?\n",
        "- Creates Multiple Different Training Sets\n",
        "\n",
        "Bagging uses bootstrap sampling to create many different datasets by randomly sampling with replacement from the original data.\n",
        "\n",
        "Each base learner (like a decision tree) is trained on a different subset, so they see slightly different data.\n",
        "\n",
        "Trains Multiple Base Learners Independently\n",
        "\n",
        "Because the training sets differ, the models learn different patterns and make different errors.\n",
        "\n",
        "Aggregates Predictions to Smooth Out Noise\n",
        "\n",
        "For classification, Bagging uses majority voting; for regression, it averages predictions.\n",
        "\n",
        "This aggregation reduces variance ‚Äî the tendency of individual learners to overfit their training data.\n",
        "17. Why is Random Forest better than a single Decision Tree\n",
        "- 1. Reduces Overfitting / Variance\n",
        "Decision Trees tend to overfit the training data because they can create very complex trees that capture noise.\n",
        "\n",
        "Random Forest builds many trees on different random subsets of data and features, then averages their predictions.\n",
        "2. Introduces Feature Randomness\n",
        "At each split, Random Forest considers a random subset of features rather than all features.\n",
        "\n",
        "This decorrelates trees and increases diversity among them.\n",
        "\n",
        "Diverse trees mean errors aren‚Äôt correlated, so averaging smooths out individual mistakes better.\n",
        "\n",
        "3. More Robust and Stable\n",
        "Because it combines many trees, Random Forest is less sensitive to noise and outliers.\n",
        "\n",
        "Single decision trees can be unstable ‚Äî small changes in data can cause very different trees.\n",
        "\n",
        "Random Forest reduces this instability.\n",
        "18. What is the role of bootstrap sampling in Bagging ?\n",
        "- Bootstrap sampling means randomly sampling with replacement from the original dataset to create multiple new datasets (called bootstrap samples).\n",
        "\n",
        "Each bootstrap sample is typically the same size as the original dataset but contains some repeated samples and leaves out others.\n",
        "Introduces Diversity:\n",
        "Different base learners see different subsets of the data, so they learn different patterns and make different errors.\n",
        "\n",
        "Reduces Overfitting:\n",
        "Because each model is trained on a different sample, their errors are less correlated. When you aggregate their predictions (by voting or averaging), the combined model is less likely to overfit.\n",
        "19.  What are some real-world applications of ensemble techniques\n",
        "- Credit scoring & risk assessment: Combining multiple models to predict loan defaults or fraud.\n",
        "\n",
        "Algorithmic trading: Ensemble methods predict stock prices or market trends by aggregating various models.\n",
        "\n",
        "2. Healthcare\n",
        "Medical diagnosis: Ensemble models analyze medical images or patient data to detect diseases like cancer.\n",
        "\n",
        "Drug discovery: Predicting molecular properties by combining outputs from different predictive models\n",
        "20. What is the difference between Bagging and Boosting ?\n",
        "- Bagging: Build many independent models on different random subsets, then combine their outputs to reduce variance.\n",
        "\n",
        "Boosting: Build models sequentially, each correcting errors of the previous, to reduce bias and build a strong learner.\n",
        "#Practical\n",
        "1. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy ?\n",
        "-\n"
      ],
      "metadata": {
        "id": "ySho-0k0P1r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Create a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize base estimator\n",
        "base_dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Initialize Bagging Classifier with Decision Trees\n",
        "bagging_clf = BaggingClassifier(base_estimator=base_dt, n_estimators=50, random_state=42)\n",
        "\n",
        "# 5. Train the Bagging Classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predict on test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# 7. Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "aXgS5TP9a91q",
        "outputId": "d090baa8-ecbe-494e-b963-c3b3a4925a82"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1758871705.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 4. Initialize Bagging Classifier with Decision Trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mbagging_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_dt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# 5. Train the Bagging Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE) ?\n"
      ],
      "metadata": {
        "id": "4c6KNKpGa_27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Create a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=10.0, random_state=42)\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize base estimator\n",
        "base_dt = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# 4. Initialize Bagging Regressor with Decision Trees\n",
        "bagging_reg = BaggingRegressor(base_estimator=base_dt, n_estimators=50, random_state=42)\n",
        "\n",
        "# 5. Train the Bagging Regressor\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predict on test set\n",
        "y_pred = bagging_reg.predict(X_t_\n"
      ],
      "metadata": {
        "id": "GdlwfqrcbJNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores"
      ],
      "metadata": {
        "id": "nWR8cvv6bOFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize and train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Get feature importance scores\n",
        "importances = rf_clf.feature_importances_\n",
        "\n",
        "# 5. Print feature importance scores alongside feature names\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "gjDyMsiLbS7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Train a Random Forest Regressor and compare its performance with a single Decision Tree ?"
      ],
      "metadata": {
        "id": "BTUsl_YTbXgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=15.0, random_state=42)\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize and train a single Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train, y_tr_\n"
      ],
      "metadata": {
        "id": "C6GkC2hQbcqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier ?"
      ],
      "metadata": {
        "id": "pMdHNOdRbhUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Initialize Random Forest with OOB enabled\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "\n",
        "# Train model\n",
        "rf_clf.fit(X, y)\n",
        "\n",
        "# Print OOB score\n",
        "print(f\"OOB Score: {rf_clf.oob_score_:.4f}\")\n"
      ],
      "metadata": {
        "id": "IFWF7X5Fbncp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Train a Bagging Classifier using SVM as a base estimator and print accuracy"
      ],
      "metadata": {
        "id": "kTjm7N1vbsBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Create a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42)\n",
        "\n",
        "# 2. Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize SVM base estimator\n",
        "svm_base = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "\n",
        "# 4. Initialize Bagging Classifier with SVM base estimator\n",
        "bagging_svm = BaggingClassifier(base_estimator=svm_base, n_estimators=20, random_state=42)\n",
        "\n",
        "# 5. Train Bagging Classifier\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predict on test set\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# 7. Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier with SVM base estimator Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "EWNoW9uobxEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Train a Random Forest Classifier with different numbers of trees and compare accuracy ?"
      ],
      "metadata": {
        "id": "knJxMT_kb35I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different numbers of trees to try\n",
        "n_estimators_list = [1, 5, 10, 50, 100, 200]\n",
        "\n",
        "print(\"Number of Trees | Accuracy\")\n",
        "print(\"----------------|---------\")\n",
        "\n",
        "for n_trees in n_estimators_list:\n",
        "    # Initialize Random Forest with n_trees\n",
        "    rf_clf = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and evaluate accuracy\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"{n_trees:15} | {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "dBpG8ZmHb9E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score"
      ],
      "metadata": {
        "id": "c4zXJh9ncChH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 1. Create a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42)\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize Logistic Regression base estimator\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# 4. Initialize Bagging Classifier with Logistic Regression base estimator\n",
        "bagging_clf = BaggingClassifier(base_estimator=log_reg, n_estimators=50, random_state=42)\n",
        "\n",
        "# 5. Train the Bagging Classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predict probabilities for the positive class\n",
        "y_prob = bagging_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 7. Calculate and print AUC score\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "print(f\"Bagging Classifier with Logistic Regression AUC: {auc_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "u0CjAAcGcHyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Train a Random Forest Regressor and analyze feature importance scores"
      ],
      "metadata": {
        "id": "k9ycnddhcMdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# 1. Create a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=15.0, random_state=42)\n",
        "feature_names_\n"
      ],
      "metadata": {
        "id": "Y04eCclNcRto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 Train an ensemble model using both Bagging and Random Forest and compare accuracy\n"
      ],
      "metadata": {
        "id": "YcY7h-PScWqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Classifier with Decision Trees as base estimators\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(random_state=42),\n"
      ],
      "metadata": {
        "id": "mTPcebU1ccI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV ?"
      ],
      "metadata": {
        "id": "MoNR_7Eycf_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 20],\n",
        "    'min_samples_split':\n"
      ],
      "metadata": {
        "id": "YUQmugO3cl9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Train a Bagging Regressor with different numbers of base estimators and compare performance ?\n"
      ],
      "metadata": {
        "id": "3SciNEPHcqP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=15.0, random_state=42)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different numbers of base estimators to try\n",
        "n_estimators_list = [1, 5, 10, 50, 100]\n",
        "\n",
        "print(\"Number of Estimators | Mean Squared Error\")\n",
        "print(\"---------------------|------\n"
      ],
      "metadata": {
        "id": "v2rsXLZPcyNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Train a Random Forest Classifier and analyze misclassified samples"
      ],
      "metadata": {
        "id": "lz5V0mWuczSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train,_\n"
      ],
      "metadata": {
        "id": "vg5AshPKc4fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier ?"
      ],
      "metadata": {
        "id": "TxNy47pcdAT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_tes_\n"
      ],
      "metadata": {
        "id": "H-5qO3xjdFNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Train a Random Forest Classifier and visualize the confusion matrix ?\n"
      ],
      "metadata": {
        "id": "ProrjojndJny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay.from_estimator(rf_clf, X_test, y_test,\n",
        "                                             display_labels=data.target_names,\n",
        "                                             cmap=plt.cm.Blues,\n",
        "                                             normalize=None)\n",
        "plt.title(\"Confusion Matrix - Random Forest Classifier\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "odA3xZZsdN3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
      ],
      "metadata": {
        "id": "sSNdQWR5dSME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_tes\n"
      ],
      "metadata": {
        "id": "SBQiCxcZdW82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Train a Random Forest Classifier and print the top 5 most important features"
      ],
      "metadata": {
        "id": "xOyhpHGLdaiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split dataset\n",
        "X\n",
        "\n"
      ],
      "metadata": {
        "id": "I5Fbyg7ZdlQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n"
      ],
      "metadata": {
        "id": "1wwKHVIWdou8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Bagging Classifier with Decision Tree base estimators\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-score:  {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "6cV9jmYBdutS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy ?"
      ],
      "metadata": {
        "id": "ePNDVyLTdxKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Try different max_depth values\n",
        "max_depth_values = [None, 2, 4, 6, 8, 10, 15, 20]\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "for depth in max_depth_values:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"max_depth={str(depth):>4} | Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot([str(d) for d i]()\n"
      ],
      "metadata": {
        "id": "S0_iXHtOd12V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance\n"
      ],
      "metadata": {
        "id": "jx306t8Yd6oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample regression d\n"
      ],
      "metadata": {
        "id": "4BBEr6MseAsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6r4lgfcBbEqc"
      }
    }
  ]
}